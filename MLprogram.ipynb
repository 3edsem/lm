{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e46022bd-f915-42ed-9415-c0058c6e6ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#program 10\n",
    "#\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "kmeans.fit(X_scaled)\n",
    "\n",
    "clusters = kmeans.predict(X_scaled)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[clusters == 0, 0], X_pca[clusters == 0, 1], s=50, c='red', label='Cluster 0 (Benign)')\n",
    "plt.scatter(X_pca[clusters == 1, 0], X_pca[clusters == 1, 1], s=50, c='blue', label='Cluster 1 (Malignant)')\n",
    "\n",
    "centroids = pca.transform(kmeans.cluster_centers_)\n",
    "plt.scatter(centroids[:, 0], centroids[:, 1], s=200, c='black', marker='X', label='Centroids')\n",
    "\n",
    "plt.title('K-Means Clustering on Breast Cancer Dataset')\n",
    "plt.xlabel('PCA Component 1')\n",
    "plt.ylabel('PCA Component 2')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad8759aa-1168-4483-9871-d460d3d6d376",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 9\n",
    "import numpy as np\n",
    "from sklearn.datasets import fetch_olivetti_faces\n",
    "from sklearn.model_selection import train_test_split, cross_val_score\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "data = fetch_olivetti_faces(shuffle=True, random_state=42)\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X_train, y_train)\n",
    "\n",
    "y_pred = gnb.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f'Accuracy: {accuracy * 100:.2f}%')\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, zero_division=1))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "\n",
    "cross_val_accuracy = cross_val_score(gnb, X, y, cv=5, scoring='accuracy')\n",
    "print(f'\\nCross-validation accuracy: {cross_val_accuracy.mean() * 100:.2f}%')\n",
    "\n",
    "fig, axes = plt.subplots(3, 5, figsize=(12, 8))\n",
    "for ax, image, label, prediction in zip(axes.ravel(), X_test, y_test, y_pred):\n",
    "    ax.imshow(image.reshape(64, 64), cmap=plt.cm.gray)\n",
    "    ax.set_title(f\"True: {label}, Pred: {prediction}\")\n",
    "    ax.axis('off')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b48dc1c-58be-44f8-add9-dafb96250888",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 8\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import load_breast_cancer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "data = load_breast_cancer()\n",
    "X = data.data\n",
    "y = data.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy of Decision Tree on Test Set: {accuracy * 100:.2f}%\")\n",
    "\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "plot_tree(clf, filled=True, feature_names=data.feature_names, class_names=data.target_names, rounded=True)\n",
    "plt.title(\"Decision Tree Visualized\")\n",
    "plt.show()\n",
    "\n",
    "new_sample = np.array([[15.2, 19.5, 103.2, 800.2, 0.07, 0.3, 1.2, 4.2, 0.03, 0.1, 0.08, 0.05, 0.03, 0.03, 0.06,\n",
    "0.09, 0.02, 0.07, 0.03, 0.08, 0.04, 0.02, 0.01, 0.04, 0.06, 0.03, 0.02, 0.01, 0.03, 0.02]])\n",
    "\n",
    "new_prediction = clf.predict(new_sample)\n",
    "print(\"\\nNew sample classified as:\", data.target_names[new_prediction][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5545a60f-d218-4d19-b99a-5cca0732bc4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 6\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def locally_weighted_regression(X_train, Y_train, X_query, tau=0.1):\n",
    "    m, n = X_train.shape\n",
    "    Y_train = Y_train.reshape(-1, 1)\n",
    "\n",
    "    X_train = np.concatenate([np.ones((m, 1)), X_train], axis=1)\n",
    "    X_query = np.concatenate([np.ones((X_query.shape[0], 1)), X_query], axis=1)\n",
    "\n",
    "    Y_pred = np.zeros(X_query.shape[0])\n",
    "\n",
    "    for i in range(X_query.shape[0]):\n",
    "        weights = np.exp(-np.sum((X_train - X_query[i])**2, axis=1) / (2 * tau**2))\n",
    "        W = np.diag(weights)\n",
    "\n",
    "        theta = np.linalg.inv(X_train.T @ W @ X_train) @ (X_train.T @ W @ Y_train)\n",
    "\n",
    "        Y_pred[i] = X_query[i] @ theta\n",
    "\n",
    "    return Y_pred\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.sort(np.random.rand(100, 1), axis=0)\n",
    "Y = np.sin(2 * np.pi * X) + 0.1 * np.random.randn(100, 1)\n",
    "\n",
    "X_query = np.linspace(0, 1, 100).reshape(-1, 1)\n",
    "\n",
    "tau_values = [0.1, 0.3, 0.5, 1.0, 2.0]\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "for tau in tau_values:\n",
    "    Y_pred = locally_weighted_regression(X, Y, X_query, tau)\n",
    "    plt.plot(X_query, Y_pred, label=f'tau={tau}')\n",
    "\n",
    "plt.scatter(X, Y, color='red', label='Data points')\n",
    "plt.xlabel('X')\n",
    "plt.ylabel('Y')\n",
    "plt.title('Locally Weighted Regression')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584efa8f-7955-4490-9f89-378a0f283b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 7\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "def linear_regression_california():\n",
    "    housing = fetch_california_housing(as_frame=True)\n",
    "    X = housing.data[[\"AveRooms\"]]\n",
    "    y = housing.target\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    model = LinearRegression()\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = model.predict(X_test)\n",
    "\n",
    "    plt.scatter(X_test, y_test, color=\"blue\", label=\"Actual\")\n",
    "    plt.plot(X_test, y_pred, color=\"red\", label=\"Predicted\")\n",
    "    plt.xlabel(\"Average number of rooms (AveRooms)\")\n",
    "    plt.ylabel(\"Median value of homes ($100,000)\")\n",
    "    plt.title(\"Linear Regression - California Housing Dataset\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Linear Regression - California Housing Dataset\")\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"R^2 Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "def polynomial_regression_auto_mpg():\n",
    "    url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/auto-mpg/auto-mpg.data\"\n",
    "    column_names = [\"mpg\", \"cylinders\", \"displacement\", \"horsepower\", \"weight\", \"acceleration\",\n",
    "                    \"model_year\", \"origin\"]\n",
    "    data = pd.read_csv(url, sep='\\s+', names=column_names, na_values=\"?\")\n",
    "    data = data.dropna()\n",
    "\n",
    "    X = data[\"displacement\"].values.reshape(-1, 1)\n",
    "    y = data[\"mpg\"].values\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "    poly_model = make_pipeline(PolynomialFeatures(degree=2), StandardScaler(), LinearRegression())\n",
    "    poly_model.fit(X_train, y_train)\n",
    "\n",
    "    y_pred = poly_model.predict(X_test)\n",
    "\n",
    "    plt.scatter(X_test, y_test, color=\"blue\", label=\"Actual\")\n",
    "    plt.scatter(X_test, y_pred, color=\"red\", label=\"Predicted\")\n",
    "    plt.xlabel(\"Displacement\")\n",
    "    plt.ylabel(\"Miles per gallon (mpg)\")\n",
    "    plt.title(\"Polynomial Regression - Auto MPG Dataset\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    print(\"Polynomial Regression - Auto MPG Dataset\")\n",
    "    print(\"Mean Squared Error:\", mean_squared_error(y_test, y_pred))\n",
    "    print(\"R^2 Score:\", r2_score(y_test, y_pred))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Demonstrating Linear Regression and Polynomial Regression\\n\")\n",
    "    linear_regression_california()\n",
    "    polynomial_regression_auto_mpg()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10262cf9-c1b3-42f7-a617-4132e013e7ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 5 \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "np.random.seed(42)\n",
    "X = np.random.rand(100, 1)\n",
    "\n",
    "y = np.zeros(100)\n",
    "for i in range(50):\n",
    "    if X[i] <= 0.5:\n",
    "        y[i] = 1\n",
    "    else:\n",
    "        y[i] = 2\n",
    "\n",
    "X_train = X[:50]\n",
    "y_train = y[:50]\n",
    "X_test = X[50:]\n",
    "\n",
    "k_values = [1, 2, 3, 4, 5, 20, 30]\n",
    "accuracies = []\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred = knn.predict(X_test)\n",
    "    accuracy = accuracy_score(y[50:], y_pred)\n",
    "    accuracies.append(accuracy)\n",
    "    print(f'Accuracy for k={k}: {accuracy:.4f}')\n",
    "\n",
    "plt.figure(figsize=(12, 10))\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "\n",
    "for k, accuracy in zip(k_values, accuracies):\n",
    "    plt.scatter(X_test, y[50:], label=f'Test Data (k={k}, Accuracy={accuracy:.4f})')\n",
    "\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Classes')\n",
    "plt.title('k-NN Classification for Randomly Generated Data')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "X_full = np.linspace(0, 1, 500).reshape(-1, 1)\n",
    "plt.figure(figsize=(12, 10))\n",
    "\n",
    "for k in k_values:\n",
    "    knn = KNeighborsClassifier(n_neighbors=k)\n",
    "    knn.fit(X_train, y_train)\n",
    "    y_pred_full = knn.predict(X_full)\n",
    "    plt.plot(X_full, y_pred_full, label=f'Decision Boundary for k={k}')\n",
    "\n",
    "plt.scatter(X_train, y_train, color='blue', label='Training Data')\n",
    "plt.scatter(X_test, y[50:], color='red', label='Test Data')\n",
    "\n",
    "plt.xlabel('X values')\n",
    "plt.ylabel('Classes')\n",
    "plt.title('Decision Boundaries for Different k in k-NN')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a58fd1-e828-4e9a-b208-3b118e0b8cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 4 \n",
    "import pandas as pd\n",
    "\n",
    "def find_s_algorithm(file_path):\n",
    "    data = pd.read_csv(file_path)\n",
    "    print(\"Training data:\")\n",
    "    print(data)\n",
    "\n",
    "    attributes = data.columns[:-1]\n",
    "    class_label = data.columns[-1]\n",
    "    hypothesis = ['?' for _ in attributes]\n",
    "\n",
    "    for index, row in data.iterrows():\n",
    "        if row[class_label] == 'Yes':\n",
    "            for i, value in enumerate(row[attributes]):\n",
    "                if hypothesis[i] == '?' or hypothesis[i] == value:\n",
    "                    hypothesis[i] = value\n",
    "                else:\n",
    "                    hypothesis[i] = '?'\n",
    "\n",
    "    return hypothesis\n",
    "\n",
    "file_path = 'training_data.csv'\n",
    "hypothesis = find_s_algorithm(file_path)\n",
    "print(\"\\nThe final hypothesis is:\", hypothesis)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467c85bf-a21e-488b-9edb-2c93d5a73f08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 3\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "iris = load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.scatter(X_pca[:, 0], X_pca[:, 1], c=y, cmap='viridis', edgecolor='k', s=50)\n",
    "plt.xlabel('Principal Component 1')\n",
    "plt.ylabel('Principal Component 2')\n",
    "plt.title('PCA - Iris Dataset (Reduced to 2 Dimensions)')\n",
    "plt.colorbar(label='Iris Species')\n",
    "plt.show()\n",
    "\n",
    "print('Explained variance ratio for each component:', pca.explained_variance_ratio_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd07d90-cbeb-4608-b24a-830d8e1a5896",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "df = california_housing.frame\n",
    "\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', fmt=\".2f\", linewidths=0.5)\n",
    "plt.title('Correlation Matrix of Features')\n",
    "plt.show()\n",
    "\n",
    "subset_features = df.columns[:5]\n",
    "sns.pairplot(df[subset_features], hue=None, plot_kws={'alpha': 0.7})\n",
    "plt.suptitle('Pair Plot of Selected Features', y=1.02)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d00ffa52-5dc0-46ae-9722-3ec0766d01c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#program 1 \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "california_housing = fetch_california_housing(as_frame=True)\n",
    "df = california_housing.frame\n",
    "\n",
    "numerical_features = df.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.histplot(df[feature], kde=True, bins=30)\n",
    "    plt.title(f'Distribution of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "    plt.ylabel('Frequency')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "for i, feature in enumerate(numerical_features, 1):\n",
    "    plt.subplot(3, 3, i)\n",
    "    sns.boxplot(x=df[feature])\n",
    "    plt.title(f'Box Plot of {feature}')\n",
    "    plt.xlabel(feature)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "for feature in numerical_features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    outliers = df[(df[feature] < lower_bound) | (df[feature] > upper_bound)]\n",
    "    print(f'Feature: {feature}')\n",
    "    print(f'  Q1: {Q1}, Q3: {Q3}, IQR: {IQR}')\n",
    "    print(f'  Lower Bound: {lower_bound}, Upper Bound: {upper_bound}')\n",
    "    print(f'  Number of outliers: {outliers.shape[0]}')\n",
    "    print('---')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
